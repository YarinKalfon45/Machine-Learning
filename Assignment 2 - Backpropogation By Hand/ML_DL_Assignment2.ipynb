{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhfIKbQzGq2"
      },
      "source": [
        "# Assignment 2. Music Century Classification\n",
        "#### Yarin Kalfon \n",
        "For this task, we will construct models to predict the century in which a music piece was released. We will utilize the \"YearPredictionMSD Data Set,\" which is derived from the Million Song Dataset from the UCI Machine Learning Repository. Make sure you download the version of the dataset from the moodle and not from UCI. Here are some relevant links to read on this dataset:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "Just like in the last assignment, it is divided to two files.\n",
        "1. This file (ML_DL_Assignment2.ipynb)\n",
        "2. A python functions  file which you will fill out (ML_DL_Functions2.py)\n",
        "\n",
        "As well as the year prediction msd dataset file.\n",
        "\n",
        "In this assignment you will mount and load the dataset and functions file from google drive. To start make sure you have both the template python functions file and the song dataset file(downloaded from the moodle) on the same directory in your google drive.\n",
        "\n",
        "When you are finished with the assignment make sure you submit the following files:\n",
        "1. this file (ML_DL_Assignment2.ipynb).\n",
        "2. the functions file (ML_DL_Functions2.py).\n",
        "3. the weights file from section 2.7 (assignment2_submission_optimal_weights.npy).\n",
        "4. the bias file from section 2.7 (assignment2_submission_optimal_bias.npy).\n",
        "\n",
        "\n",
        "Note that untill section 2.9 you are not allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us. Importing the pytorch package will deduct from your points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oq1vy5PUIV"
      },
      "source": [
        "## 1. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9",
        "ExecuteTime": {
          "end_time": "2023-11-08T13:49:56.865016300Z",
          "start_time": "2023-11-08T13:49:56.859011700Z"
        }
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "def reload_functions():\n",
        "  if 'ML_DL_Functions2' in sys.modules:\n",
        "    del sys.modules['ML_DL_Functions2']\n",
        "  functions_path = drive_path.replace(\" \",\"\\ \") + 'ML_DL_Functions2.py'\n",
        "  !cp $functions_path ."
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7UWL6mFzGq-"
      },
      "source": [
        "Just like in the last assignment you should mount your google drive and make sure you have both the dataset from the moodle('YearPredictionMSD.csv') and the functions file ('ML_DL_Functions2.py') in the same directory which you will input below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6PrfV4zGq_",
        "ExecuteTime": {
          "end_time": "2023-11-08T13:50:00.601972300Z",
          "start_time": "2023-11-08T13:50:00.356419700Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6b7ee8-f577-494b-fdd0-7250c7489c5b"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive_path = '/content/gdrive/My Drive/Intro_to_Deep_Learning/Assignment2/' # TODO - UPDATE ME WITH THE TRUE PATH!\n",
        "csv_path = drive_path + 'YearPredictionMSD.csv'\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgB83beNzGq_"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bBEnj3zGq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "6e5883c0-ef78-45d9-cada-2670e264d025"
      },
      "source": [
        "df"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        year      var1      var2      var3      var4      var5      var6  \\\n",
              "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "...      ...       ...       ...       ...       ...       ...       ...   \n",
              "508710  1979  42.88385 -17.41629 -13.51726  -0.75243   4.74785  14.33437   \n",
              "508711  2010  42.47120  13.16539  -6.89795  14.78750  14.72776 -12.05820   \n",
              "508712  2010  45.21104 -19.03522 -16.50919  19.30722 -22.23290 -25.77296   \n",
              "508713  2004  44.60991  29.26510 -14.79970  16.26654 -20.44287  34.93228   \n",
              "508714  2002  51.58607  62.50479  10.33764  -7.60272 -35.88361 -30.27671   \n",
              "\n",
              "            var7      var8      var9  ...     var81      var82     var83  \\\n",
              "0      -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548  58.99367   \n",
              "1        8.76630  -0.92019  18.76548  ...   5.66812  -19.68073  33.04964   \n",
              "2       -3.27872  -2.35035  16.07017  ...   3.03800   26.05866 -50.92779   \n",
              "3        5.05097 -10.34124   3.55005  ...  34.57337 -171.70734 -16.96705   \n",
              "4      -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724  64.92712   \n",
              "...          ...       ...       ...  ...       ...        ...       ...   \n",
              "508710 -11.75670 -10.46058 -14.91937  ... -23.01045 -169.62524  43.90683   \n",
              "508711  -6.56437  -7.70141  -8.01135  ...   6.97510   97.98602 -45.39312   \n",
              "508712  15.66504  -3.26132   1.78980  ...  17.64373   27.46728  48.64159   \n",
              "508713  -8.15282   2.94035  -1.93460  ...  21.30827 -183.32526 -40.60815   \n",
              "508714   7.36746   0.33782   5.51725  ...   0.57308   34.69757 -22.81646   \n",
              "\n",
              "           var84     var85      var86      var87     var88      var89  \\\n",
              "0       15.37344   1.11144  -23.08793   68.40795  -1.82223  -27.46348   \n",
              "1       42.87836  -9.90378  -32.22788   70.49388  12.04941   58.43453   \n",
              "2       10.93792  -0.07568   43.20130 -115.00698  -0.05859   39.67068   \n",
              "3      -46.67617 -12.51516   82.58061  -72.08993   9.90558  199.62971   \n",
              "4      -17.72522  -1.49237   -7.50035   51.76631   7.88713   55.66926   \n",
              "...          ...       ...        ...        ...       ...        ...   \n",
              "508710  15.45299   2.84499   94.83469 -157.26665   3.60034   54.26775   \n",
              "508711 -30.26953  -9.49116  -51.58060  -12.08770   0.10696  117.82374   \n",
              "508712  92.03877  11.31597 -189.77886  179.06219  -3.74635  -27.01421   \n",
              "508713  19.53727  12.13429 -133.10456 -158.46478  22.36919  161.58392   \n",
              "508714  27.20861  -1.22381   -8.51102  -18.26440   2.22031  -93.94392   \n",
              "\n",
              "           var90  \n",
              "0        2.26327  \n",
              "1       26.92061  \n",
              "2       -0.66345  \n",
              "3       18.85382  \n",
              "4       28.74903  \n",
              "...          ...  \n",
              "508710 -22.24375  \n",
              "508711  -1.06577  \n",
              "508712 -10.23084  \n",
              "508713 -18.54131  \n",
              "508714  -6.75431  \n",
              "\n",
              "[508715 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bb07b499-4f5a-4313-99ee-f3f684f07f45\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508710</th>\n",
              "      <td>1979</td>\n",
              "      <td>42.88385</td>\n",
              "      <td>-17.41629</td>\n",
              "      <td>-13.51726</td>\n",
              "      <td>-0.75243</td>\n",
              "      <td>4.74785</td>\n",
              "      <td>14.33437</td>\n",
              "      <td>-11.75670</td>\n",
              "      <td>-10.46058</td>\n",
              "      <td>-14.91937</td>\n",
              "      <td>...</td>\n",
              "      <td>-23.01045</td>\n",
              "      <td>-169.62524</td>\n",
              "      <td>43.90683</td>\n",
              "      <td>15.45299</td>\n",
              "      <td>2.84499</td>\n",
              "      <td>94.83469</td>\n",
              "      <td>-157.26665</td>\n",
              "      <td>3.60034</td>\n",
              "      <td>54.26775</td>\n",
              "      <td>-22.24375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508711</th>\n",
              "      <td>2010</td>\n",
              "      <td>42.47120</td>\n",
              "      <td>13.16539</td>\n",
              "      <td>-6.89795</td>\n",
              "      <td>14.78750</td>\n",
              "      <td>14.72776</td>\n",
              "      <td>-12.05820</td>\n",
              "      <td>-6.56437</td>\n",
              "      <td>-7.70141</td>\n",
              "      <td>-8.01135</td>\n",
              "      <td>...</td>\n",
              "      <td>6.97510</td>\n",
              "      <td>97.98602</td>\n",
              "      <td>-45.39312</td>\n",
              "      <td>-30.26953</td>\n",
              "      <td>-9.49116</td>\n",
              "      <td>-51.58060</td>\n",
              "      <td>-12.08770</td>\n",
              "      <td>0.10696</td>\n",
              "      <td>117.82374</td>\n",
              "      <td>-1.06577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508712</th>\n",
              "      <td>2010</td>\n",
              "      <td>45.21104</td>\n",
              "      <td>-19.03522</td>\n",
              "      <td>-16.50919</td>\n",
              "      <td>19.30722</td>\n",
              "      <td>-22.23290</td>\n",
              "      <td>-25.77296</td>\n",
              "      <td>15.66504</td>\n",
              "      <td>-3.26132</td>\n",
              "      <td>1.78980</td>\n",
              "      <td>...</td>\n",
              "      <td>17.64373</td>\n",
              "      <td>27.46728</td>\n",
              "      <td>48.64159</td>\n",
              "      <td>92.03877</td>\n",
              "      <td>11.31597</td>\n",
              "      <td>-189.77886</td>\n",
              "      <td>179.06219</td>\n",
              "      <td>-3.74635</td>\n",
              "      <td>-27.01421</td>\n",
              "      <td>-10.23084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508713</th>\n",
              "      <td>2004</td>\n",
              "      <td>44.60991</td>\n",
              "      <td>29.26510</td>\n",
              "      <td>-14.79970</td>\n",
              "      <td>16.26654</td>\n",
              "      <td>-20.44287</td>\n",
              "      <td>34.93228</td>\n",
              "      <td>-8.15282</td>\n",
              "      <td>2.94035</td>\n",
              "      <td>-1.93460</td>\n",
              "      <td>...</td>\n",
              "      <td>21.30827</td>\n",
              "      <td>-183.32526</td>\n",
              "      <td>-40.60815</td>\n",
              "      <td>19.53727</td>\n",
              "      <td>12.13429</td>\n",
              "      <td>-133.10456</td>\n",
              "      <td>-158.46478</td>\n",
              "      <td>22.36919</td>\n",
              "      <td>161.58392</td>\n",
              "      <td>-18.54131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508714</th>\n",
              "      <td>2002</td>\n",
              "      <td>51.58607</td>\n",
              "      <td>62.50479</td>\n",
              "      <td>10.33764</td>\n",
              "      <td>-7.60272</td>\n",
              "      <td>-35.88361</td>\n",
              "      <td>-30.27671</td>\n",
              "      <td>7.36746</td>\n",
              "      <td>0.33782</td>\n",
              "      <td>5.51725</td>\n",
              "      <td>...</td>\n",
              "      <td>0.57308</td>\n",
              "      <td>34.69757</td>\n",
              "      <td>-22.81646</td>\n",
              "      <td>27.20861</td>\n",
              "      <td>-1.22381</td>\n",
              "      <td>-8.51102</td>\n",
              "      <td>-18.26440</td>\n",
              "      <td>2.22031</td>\n",
              "      <td>-93.94392</td>\n",
              "      <td>-6.75431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>508715 rows × 91 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb07b499-4f5a-4313-99ee-f3f684f07f45')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bb07b499-4f5a-4313-99ee-f3f684f07f45 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bb07b499-4f5a-4313-99ee-f3f684f07f45');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-63bcd002-a45e-46d8-84be-934a245d3f42\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-63bcd002-a45e-46d8-84be-934a245d3f42')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-63bcd002-a45e-46d8-84be-934a245d3f42 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_36fd150d-1e03-43e8-87b1-6753498b2bf5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_36fd150d-1e03-43e8-87b1-6753498b2bf5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaLuAMH_zGrA"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZdGlNgdzGrA"
      },
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugy7FZ8eoAd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "outputId": "e86faf6b-b77c-477c-a2c7-458100cb7127"
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    year      var1       var2      var3      var4      var5      var6  \\\n",
              "0      1  49.94357   21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1      1  48.73215   18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2      1  50.95714   31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3      1  48.24750   -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4      1  50.97020   42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "5      1  50.54767    0.31568  92.35066  22.38696 -25.51870 -19.04928   \n",
              "6      1  50.57546   33.17843  50.53517  11.55217 -27.24764  -8.78206   \n",
              "7      1  48.26892    8.97526  75.23158  24.04945 -16.02105 -14.09491   \n",
              "8      1  49.75468   33.99581  56.73846   2.89581  -2.92429 -26.44413   \n",
              "9      1  45.17809   46.34234 -40.65357  -2.47909   1.21253  -0.65302   \n",
              "10     1  39.13076  -23.01763 -36.20583   1.67519  -4.27101  13.01158   \n",
              "11     1  37.66498  -34.05910 -17.36060 -26.77781 -39.95119 -20.75000   \n",
              "12     1  26.51957 -148.15762 -13.30095  -7.25851  17.22029 -21.99439   \n",
              "13     1  37.68491  -26.84185 -27.10566 -14.95883  -5.87200 -21.68979   \n",
              "14     0  39.11695   -8.29767 -51.37966  -4.42668 -30.06506 -11.95916   \n",
              "15     1  35.05129  -67.97714 -14.20239  -6.68696  -0.61230 -18.70341   \n",
              "16     1  33.63129  -96.14912 -89.38216 -12.11699  13.77252  -6.69377   \n",
              "17     0  41.38639  -20.78665  51.80155  17.21415 -36.44189 -11.53169   \n",
              "18     0  37.45034   11.42615  56.28982  19.58426 -16.43530   2.22457   \n",
              "19     0  39.71092   -4.92800  12.88590 -11.87773   2.48031 -16.11028   \n",
              "\n",
              "        var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0  -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1    8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2   -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3    5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4  -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "5   20.67345  -5.19943   3.63566  ...   6.59753  -50.69577   26.02574   \n",
              "6  -12.04282  -9.53930  28.61811  ...  11.63681   25.44182  134.62382   \n",
              "7    8.11871  -1.87566   7.46701  ...  18.03989  -58.46192  -65.56438   \n",
              "8    1.71392  -0.55644  22.08594  ...  18.70812    5.20391  -27.75192   \n",
              "9   -6.95536 -12.20040  17.02512  ...  -4.36742  -87.55285  -70.79677   \n",
              "10   8.05718  -8.41088   6.27370  ...  32.86051  -26.08461 -186.82429   \n",
              "11  -0.10231  -0.89972  -1.30205  ...  11.18909   45.20614   53.83925   \n",
              "12   5.51947   3.48418   2.61738  ...  23.80442  251.76360   18.81642   \n",
              "13   4.87374 -18.01800   1.52141  ... -67.57637  234.27192  -72.34557   \n",
              "14  -0.85322  -8.86179  11.36680  ...  42.22923  478.26580  -10.33823   \n",
              "15  -1.31928  -9.46370   5.53492  ...  10.25585   94.90539   15.95689   \n",
              "16 -33.36843 -24.81437  21.22757  ...  49.93249  -14.47489   40.70590   \n",
              "17  11.75252  -7.62428  -3.65488  ...  50.37614  -40.48205   48.07805   \n",
              "18   1.02668  -7.34736  -0.01184  ... -22.46207  -25.77228 -322.42841   \n",
              "19 -16.40421  -8.29657   9.86817  ...  11.92816  -73.72412   16.19039   \n",
              "\n",
              "        var84     var85      var86      var87     var88       var89     var90  \n",
              "0    15.37344   1.11144  -23.08793   68.40795  -1.82223   -27.46348   2.26327  \n",
              "1    42.87836  -9.90378  -32.22788   70.49388  12.04941    58.43453  26.92061  \n",
              "2    10.93792  -0.07568   43.20130 -115.00698  -0.05859    39.67068  -0.66345  \n",
              "3   -46.67617 -12.51516   82.58061  -72.08993   9.90558   199.62971  18.85382  \n",
              "4   -17.72522  -1.49237   -7.50035   51.76631   7.88713    55.66926  28.74903  \n",
              "5    18.94430  -0.33730    6.09352   35.18381   5.00283   -11.02257   0.02263  \n",
              "6    21.51982   8.17570   35.46251   11.57736   4.50056    -4.62739   1.40192  \n",
              "7    46.99856  -4.09602   56.37650  -18.29975  -0.30633     3.98364  -3.72556  \n",
              "8    17.22100  -0.85210  -15.67150  -26.36257   5.48708    -9.13495   6.08680  \n",
              "9    76.57355  -7.71727    3.26926 -298.49845  11.49326   -89.21804 -15.09719  \n",
              "10  113.58176   9.28727   44.60282  158.00425  -2.59543   109.19723  23.36143  \n",
              "11    2.59467  -4.00958  -47.74886 -170.92864  -5.19009     8.83617  -7.16056  \n",
              "12  157.09656 -27.79449 -137.72740  115.28414  23.00230  -164.02536  51.54138  \n",
              "13 -362.25101 -25.55019  -89.08971 -891.58937  14.11648 -1030.99180  99.28967  \n",
              "14 -103.76858  39.19511  -98.76636 -122.81061  -2.14942  -211.48202 -12.81569  \n",
              "15  -98.15732  -9.64859  -93.52834  -95.82981  20.73063  -562.07671  43.44696  \n",
              "16   58.63692   8.81522   27.28474    5.78046   3.44539   259.10825  10.28525  \n",
              "17   -7.62399   6.51934  -30.46090  -53.87264   4.44627    58.16913  -0.02409  \n",
              "18 -146.57408  13.61588   92.22918 -439.80259  25.73235   157.22967  38.70617  \n",
              "19    9.79606   9.71693   -9.90907  -20.65851   2.34002   -31.57015   1.58400  \n",
              "\n",
              "[20 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c149f7d-5394-4dd0-99af-3149d18e27e3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>50.54767</td>\n",
              "      <td>0.31568</td>\n",
              "      <td>92.35066</td>\n",
              "      <td>22.38696</td>\n",
              "      <td>-25.51870</td>\n",
              "      <td>-19.04928</td>\n",
              "      <td>20.67345</td>\n",
              "      <td>-5.19943</td>\n",
              "      <td>3.63566</td>\n",
              "      <td>...</td>\n",
              "      <td>6.59753</td>\n",
              "      <td>-50.69577</td>\n",
              "      <td>26.02574</td>\n",
              "      <td>18.94430</td>\n",
              "      <td>-0.33730</td>\n",
              "      <td>6.09352</td>\n",
              "      <td>35.18381</td>\n",
              "      <td>5.00283</td>\n",
              "      <td>-11.02257</td>\n",
              "      <td>0.02263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>50.57546</td>\n",
              "      <td>33.17843</td>\n",
              "      <td>50.53517</td>\n",
              "      <td>11.55217</td>\n",
              "      <td>-27.24764</td>\n",
              "      <td>-8.78206</td>\n",
              "      <td>-12.04282</td>\n",
              "      <td>-9.53930</td>\n",
              "      <td>28.61811</td>\n",
              "      <td>...</td>\n",
              "      <td>11.63681</td>\n",
              "      <td>25.44182</td>\n",
              "      <td>134.62382</td>\n",
              "      <td>21.51982</td>\n",
              "      <td>8.17570</td>\n",
              "      <td>35.46251</td>\n",
              "      <td>11.57736</td>\n",
              "      <td>4.50056</td>\n",
              "      <td>-4.62739</td>\n",
              "      <td>1.40192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>48.26892</td>\n",
              "      <td>8.97526</td>\n",
              "      <td>75.23158</td>\n",
              "      <td>24.04945</td>\n",
              "      <td>-16.02105</td>\n",
              "      <td>-14.09491</td>\n",
              "      <td>8.11871</td>\n",
              "      <td>-1.87566</td>\n",
              "      <td>7.46701</td>\n",
              "      <td>...</td>\n",
              "      <td>18.03989</td>\n",
              "      <td>-58.46192</td>\n",
              "      <td>-65.56438</td>\n",
              "      <td>46.99856</td>\n",
              "      <td>-4.09602</td>\n",
              "      <td>56.37650</td>\n",
              "      <td>-18.29975</td>\n",
              "      <td>-0.30633</td>\n",
              "      <td>3.98364</td>\n",
              "      <td>-3.72556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>49.75468</td>\n",
              "      <td>33.99581</td>\n",
              "      <td>56.73846</td>\n",
              "      <td>2.89581</td>\n",
              "      <td>-2.92429</td>\n",
              "      <td>-26.44413</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>-0.55644</td>\n",
              "      <td>22.08594</td>\n",
              "      <td>...</td>\n",
              "      <td>18.70812</td>\n",
              "      <td>5.20391</td>\n",
              "      <td>-27.75192</td>\n",
              "      <td>17.22100</td>\n",
              "      <td>-0.85210</td>\n",
              "      <td>-15.67150</td>\n",
              "      <td>-26.36257</td>\n",
              "      <td>5.48708</td>\n",
              "      <td>-9.13495</td>\n",
              "      <td>6.08680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>45.17809</td>\n",
              "      <td>46.34234</td>\n",
              "      <td>-40.65357</td>\n",
              "      <td>-2.47909</td>\n",
              "      <td>1.21253</td>\n",
              "      <td>-0.65302</td>\n",
              "      <td>-6.95536</td>\n",
              "      <td>-12.20040</td>\n",
              "      <td>17.02512</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.36742</td>\n",
              "      <td>-87.55285</td>\n",
              "      <td>-70.79677</td>\n",
              "      <td>76.57355</td>\n",
              "      <td>-7.71727</td>\n",
              "      <td>3.26926</td>\n",
              "      <td>-298.49845</td>\n",
              "      <td>11.49326</td>\n",
              "      <td>-89.21804</td>\n",
              "      <td>-15.09719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>39.13076</td>\n",
              "      <td>-23.01763</td>\n",
              "      <td>-36.20583</td>\n",
              "      <td>1.67519</td>\n",
              "      <td>-4.27101</td>\n",
              "      <td>13.01158</td>\n",
              "      <td>8.05718</td>\n",
              "      <td>-8.41088</td>\n",
              "      <td>6.27370</td>\n",
              "      <td>...</td>\n",
              "      <td>32.86051</td>\n",
              "      <td>-26.08461</td>\n",
              "      <td>-186.82429</td>\n",
              "      <td>113.58176</td>\n",
              "      <td>9.28727</td>\n",
              "      <td>44.60282</td>\n",
              "      <td>158.00425</td>\n",
              "      <td>-2.59543</td>\n",
              "      <td>109.19723</td>\n",
              "      <td>23.36143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>37.66498</td>\n",
              "      <td>-34.05910</td>\n",
              "      <td>-17.36060</td>\n",
              "      <td>-26.77781</td>\n",
              "      <td>-39.95119</td>\n",
              "      <td>-20.75000</td>\n",
              "      <td>-0.10231</td>\n",
              "      <td>-0.89972</td>\n",
              "      <td>-1.30205</td>\n",
              "      <td>...</td>\n",
              "      <td>11.18909</td>\n",
              "      <td>45.20614</td>\n",
              "      <td>53.83925</td>\n",
              "      <td>2.59467</td>\n",
              "      <td>-4.00958</td>\n",
              "      <td>-47.74886</td>\n",
              "      <td>-170.92864</td>\n",
              "      <td>-5.19009</td>\n",
              "      <td>8.83617</td>\n",
              "      <td>-7.16056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>26.51957</td>\n",
              "      <td>-148.15762</td>\n",
              "      <td>-13.30095</td>\n",
              "      <td>-7.25851</td>\n",
              "      <td>17.22029</td>\n",
              "      <td>-21.99439</td>\n",
              "      <td>5.51947</td>\n",
              "      <td>3.48418</td>\n",
              "      <td>2.61738</td>\n",
              "      <td>...</td>\n",
              "      <td>23.80442</td>\n",
              "      <td>251.76360</td>\n",
              "      <td>18.81642</td>\n",
              "      <td>157.09656</td>\n",
              "      <td>-27.79449</td>\n",
              "      <td>-137.72740</td>\n",
              "      <td>115.28414</td>\n",
              "      <td>23.00230</td>\n",
              "      <td>-164.02536</td>\n",
              "      <td>51.54138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>37.68491</td>\n",
              "      <td>-26.84185</td>\n",
              "      <td>-27.10566</td>\n",
              "      <td>-14.95883</td>\n",
              "      <td>-5.87200</td>\n",
              "      <td>-21.68979</td>\n",
              "      <td>4.87374</td>\n",
              "      <td>-18.01800</td>\n",
              "      <td>1.52141</td>\n",
              "      <td>...</td>\n",
              "      <td>-67.57637</td>\n",
              "      <td>234.27192</td>\n",
              "      <td>-72.34557</td>\n",
              "      <td>-362.25101</td>\n",
              "      <td>-25.55019</td>\n",
              "      <td>-89.08971</td>\n",
              "      <td>-891.58937</td>\n",
              "      <td>14.11648</td>\n",
              "      <td>-1030.99180</td>\n",
              "      <td>99.28967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>39.11695</td>\n",
              "      <td>-8.29767</td>\n",
              "      <td>-51.37966</td>\n",
              "      <td>-4.42668</td>\n",
              "      <td>-30.06506</td>\n",
              "      <td>-11.95916</td>\n",
              "      <td>-0.85322</td>\n",
              "      <td>-8.86179</td>\n",
              "      <td>11.36680</td>\n",
              "      <td>...</td>\n",
              "      <td>42.22923</td>\n",
              "      <td>478.26580</td>\n",
              "      <td>-10.33823</td>\n",
              "      <td>-103.76858</td>\n",
              "      <td>39.19511</td>\n",
              "      <td>-98.76636</td>\n",
              "      <td>-122.81061</td>\n",
              "      <td>-2.14942</td>\n",
              "      <td>-211.48202</td>\n",
              "      <td>-12.81569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>35.05129</td>\n",
              "      <td>-67.97714</td>\n",
              "      <td>-14.20239</td>\n",
              "      <td>-6.68696</td>\n",
              "      <td>-0.61230</td>\n",
              "      <td>-18.70341</td>\n",
              "      <td>-1.31928</td>\n",
              "      <td>-9.46370</td>\n",
              "      <td>5.53492</td>\n",
              "      <td>...</td>\n",
              "      <td>10.25585</td>\n",
              "      <td>94.90539</td>\n",
              "      <td>15.95689</td>\n",
              "      <td>-98.15732</td>\n",
              "      <td>-9.64859</td>\n",
              "      <td>-93.52834</td>\n",
              "      <td>-95.82981</td>\n",
              "      <td>20.73063</td>\n",
              "      <td>-562.07671</td>\n",
              "      <td>43.44696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>33.63129</td>\n",
              "      <td>-96.14912</td>\n",
              "      <td>-89.38216</td>\n",
              "      <td>-12.11699</td>\n",
              "      <td>13.77252</td>\n",
              "      <td>-6.69377</td>\n",
              "      <td>-33.36843</td>\n",
              "      <td>-24.81437</td>\n",
              "      <td>21.22757</td>\n",
              "      <td>...</td>\n",
              "      <td>49.93249</td>\n",
              "      <td>-14.47489</td>\n",
              "      <td>40.70590</td>\n",
              "      <td>58.63692</td>\n",
              "      <td>8.81522</td>\n",
              "      <td>27.28474</td>\n",
              "      <td>5.78046</td>\n",
              "      <td>3.44539</td>\n",
              "      <td>259.10825</td>\n",
              "      <td>10.28525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>41.38639</td>\n",
              "      <td>-20.78665</td>\n",
              "      <td>51.80155</td>\n",
              "      <td>17.21415</td>\n",
              "      <td>-36.44189</td>\n",
              "      <td>-11.53169</td>\n",
              "      <td>11.75252</td>\n",
              "      <td>-7.62428</td>\n",
              "      <td>-3.65488</td>\n",
              "      <td>...</td>\n",
              "      <td>50.37614</td>\n",
              "      <td>-40.48205</td>\n",
              "      <td>48.07805</td>\n",
              "      <td>-7.62399</td>\n",
              "      <td>6.51934</td>\n",
              "      <td>-30.46090</td>\n",
              "      <td>-53.87264</td>\n",
              "      <td>4.44627</td>\n",
              "      <td>58.16913</td>\n",
              "      <td>-0.02409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>37.45034</td>\n",
              "      <td>11.42615</td>\n",
              "      <td>56.28982</td>\n",
              "      <td>19.58426</td>\n",
              "      <td>-16.43530</td>\n",
              "      <td>2.22457</td>\n",
              "      <td>1.02668</td>\n",
              "      <td>-7.34736</td>\n",
              "      <td>-0.01184</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.46207</td>\n",
              "      <td>-25.77228</td>\n",
              "      <td>-322.42841</td>\n",
              "      <td>-146.57408</td>\n",
              "      <td>13.61588</td>\n",
              "      <td>92.22918</td>\n",
              "      <td>-439.80259</td>\n",
              "      <td>25.73235</td>\n",
              "      <td>157.22967</td>\n",
              "      <td>38.70617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>39.71092</td>\n",
              "      <td>-4.92800</td>\n",
              "      <td>12.88590</td>\n",
              "      <td>-11.87773</td>\n",
              "      <td>2.48031</td>\n",
              "      <td>-16.11028</td>\n",
              "      <td>-16.40421</td>\n",
              "      <td>-8.29657</td>\n",
              "      <td>9.86817</td>\n",
              "      <td>...</td>\n",
              "      <td>11.92816</td>\n",
              "      <td>-73.72412</td>\n",
              "      <td>16.19039</td>\n",
              "      <td>9.79606</td>\n",
              "      <td>9.71693</td>\n",
              "      <td>-9.90907</td>\n",
              "      <td>-20.65851</td>\n",
              "      <td>2.34002</td>\n",
              "      <td>-31.57015</td>\n",
              "      <td>1.58400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 91 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c149f7d-5394-4dd0-99af-3149d18e27e3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c149f7d-5394-4dd0-99af-3149d18e27e3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c149f7d-5394-4dd0-99af-3149d18e27e3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ebbed972-3baf-4d2f-9ee6-a1b200e1fb01\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ebbed972-3baf-4d2f-9ee6-a1b200e1fb01')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ebbed972-3baf-4d2f-9ee6-a1b200e1fb01 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncjxI4WdzGrA"
      },
      "source": [
        "### 1.1 - Train Test Split\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "#### Food for thought:\n",
        "why would it be problematic to have some songs from an artist in the training set, and other songs from the same artist in the test set. (Hint: Remember that we want our test accuracy to predict how well the model will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NiYlxpFzGrB"
      },
      "source": [
        "# train test split\n",
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# convert to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYSzd4XUzGrB"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "Normalize the data by subtracting the mean and dividing by the std just like the last assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPuWLksJzGrB"
      },
      "source": [
        "# Insert your code here:\n",
        "f_means = train_xs.mean(axis=0)\n",
        "f_std = train_xs.std(axis=0)\n",
        "f_std[f_std == 0] = 0.01\n",
        "train_norm_xs = (train_xs - f_means) / f_std\n",
        "test_norm_xs = (test_xs - f_means) / f_std"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GqL5J_zGrC"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "#### Food for thought:\n",
        "Why should we limit how many times we use the test set, and how do we use the validation set during the model building process?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXv1U3gzGrC"
      },
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4lt445zGrD"
      },
      "source": [
        "## Part 2. Classification (79%)\n",
        "\n",
        "We will first build a *classification* model to perform decade classification. We have written a few helper functions for you. You can find them in your functions file ('sigmoid', 'cross_entropy' and 'get_accuracy'). All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops). Feel free to add more testing to the notebook to validate your code in the functions file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZIfooBzGrD"
      },
      "source": [
        "### 2.1 Prediction\n",
        "\n",
        "Fill in the function `pred` in the functions file that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by:\n",
        "\\begin{equation}\n",
        "y = \\sigma({\\bf w}^T {\\bf x} + b),\n",
        "\\end{equation}\n",
        "where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naY5mT4_zGrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b57f75-82ce-4e8e-ce0c-c43c9d38100c"
      },
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "ML_DL_Functions2.pred(np.zeros(90), 1, np.ones([2, 90]))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.73105858, 0.73105858])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xCrT4b0jlxhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Cost\n",
        "Assuming the loss function is the cross entropy function fill in the cost(risk) function in the functions file which returns the mean of the loss function on all inputs.\n",
        "$$\\mathcal{L}_\\mathcal{P}(\\text{Cross Entropy}) = \\mathbb{E}_{(y,t)\\sim\\mathcal{P}}\\left\\{\\text{CE}(t,s)\\right\\}$$\n"
      ],
      "metadata": {
        "id": "-fOu-ekEJ0dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "print(ML_DL_Functions2.cost(0.5*np.ones(4), np.ones(4)))"
      ],
      "metadata": {
        "id": "XmWuoJocXFe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb96be99-c35d-4406-bd3d-400a4edf760b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6931471805599453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNdmSd3zGrE"
      },
      "source": [
        "### 2.3 Derivative of the cost -- 7%\n",
        "Take a pen and paper and calculate the analytical derivative of the cost function with respect to the weights and bias. use the formula calculated to fill in the function `derivative_cost` that computes and returns the gradients\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80bu7qmzGrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae66e864-8498-439e-e7d2-d08d9ec3fa3a"
      },
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "dldw, dldb = ML_DL_Functions2.derivative_cost(np.ones([10,90]), np.ones(10), np.ones(10))\n",
        "print(dldw.shape)\n",
        "print(type(dldb))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90,)\n",
            "<class 'numpy.float64'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQXAKd4zGrE"
      },
      "source": [
        "### 2.4 Derivative approximation\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "make sure that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpRTD-fozGrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "782b9e12-fae3-4002-ad1d-3d11596e8ba2"
      },
      "source": [
        "# Import your module\n",
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "\n",
        "X = train_norm_xs  # Feature matrix\n",
        "t = train_ts.flatten()  # Labels, flattened to (N,)\n",
        "W = np.random.normal(0, 0.1, size=90)  # Small random weights\n",
        "b = 0.1                                # Small initial bias\n",
        "\n",
        "# L(b)\n",
        "y_b = ML_DL_Functions2.pred(W, b, X)  # Predicted probabilities\n",
        "l_b = ML_DL_Functions2.cost(y_b, t)   # Cost with current bias\n",
        "\n",
        "# L(b + h)\n",
        "h = 1e-6\n",
        "y_plus = ML_DL_Functions2.pred(W, b + h, X)\n",
        "l_plus = ML_DL_Functions2.cost(y_plus, t)\n",
        "\n",
        "# Numerical derivative\n",
        "numerical_dldb = (l_plus - l_b) / h\n",
        "\n",
        "# Analytical derivative\n",
        "dldw, dldb = ML_DL_Functions2.derivative_cost(X, y_b, t)\n",
        "\n",
        "# Compare results\n",
        "print(\"Cost at b (l_b):\", l_b)\n",
        "print(\"Cost at b + h (l_plus):\", l_plus)\n",
        "print(\"Difference:\", np.abs(dldb - numerical_dldb))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at b (l_b): 0.8460959110841197\n",
            "Cost at b + h (l_plus): 0.8460958551740972\n",
            "Difference: 1.6804654198489255e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiplTPhzGrF"
      },
      "source": [
        "make sure that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTsHgnPzGrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b509801-1b2e-4e70-d8c2-757e72c1e6cc"
      },
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "\n",
        "X = train_norm_xs\n",
        "t = train_ts.flatten()\n",
        "W = np.random.normal(0, 0.1, size=90)\n",
        "b = 0.1\n",
        "h = 1e-6\n",
        "\n",
        "# L(W)\n",
        "y_W = ML_DL_Functions2.pred(W, b, X)\n",
        "l_W = ML_DL_Functions2.cost(y_W, t)\n",
        "dldw, dldb = ML_DL_Functions2.derivative_cost(X, y_W, t)\n",
        "\n",
        "numerical_dldw = np.zeros_like(W)\n",
        "\n",
        "for j in range(len(W)):\n",
        "    W_plus = W.copy()\n",
        "    W_plus[j] += h\n",
        "    y_plus = ML_DL_Functions2.pred(W_plus, b, X)\n",
        "    l_plus = ML_DL_Functions2.cost(y_plus, t)\n",
        "    numerical_dldw[j] = (l_plus - l_W) / h\n",
        "\n",
        "print(\"Predicted probabilities (y_W):\", y_W)\n",
        "print(\"Cost at w (l_W):\", l_W)\n",
        "print(\"Cost at w + h (l_plus):\", l_plus)\n",
        "print(\"Difference:\", np.linalg.norm(dldw - numerical_dldw))\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted probabilities (y_W): [0.55892622 0.66937213 0.33673402 ... 0.49723291 0.44358036 0.4775464 ]\n",
            "Cost at w (l_W): 0.7773985996441101\n",
            "Cost at w + h (l_plus): 0.7773986618993982\n",
            "Difference: 8.672433615098221e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBTPF_2zGrG"
      },
      "source": [
        "### 2.5 Gradient descent\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent.\n",
        "Complete the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4DEuuPzGrG"
      },
      "source": [
        "def run_gradient_descent(w0, b0, train_norm_xs, train_ts, val_norm_xs, val_ts, mu=0.1, batch_size=100, max_iters=100):\n",
        "    \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\"\"\"\n",
        "    w = w0\n",
        "    b = b0\n",
        "    iter = 0\n",
        "    max_acc = 0\n",
        "    opt_w = w\n",
        "    opt_b = b\n",
        "    cost_list = []\n",
        "    acc_list = []\n",
        "\n",
        "    while iter < max_iters:\n",
        "        # Shuffle the training set\n",
        "        reindex = np.random.permutation(len(train_norm_xs))\n",
        "        train_norm_xs_shuffled = train_norm_xs[reindex]\n",
        "        train_ts_shuffled = train_ts[reindex]\n",
        "\n",
        "        for i in range(0, len(train_norm_xs_shuffled), batch_size):  # Iterate over each minibatch\n",
        "            # Minibatch that we are working with:\n",
        "            X = train_norm_xs_shuffled[i:(i + batch_size)]\n",
        "            t = train_ts_shuffled[i:(i + batch_size), 0]\n",
        "\n",
        "            # Skip the \"last\" minibatch if its size < batch_size\n",
        "            if np.shape(X)[0] != batch_size:\n",
        "                continue\n",
        "\n",
        "            # Compute the prediction\n",
        "            y = ML_DL_Functions2.pred(w, b, X)\n",
        "\n",
        "            # Calculate gradient (backpropagate)\n",
        "            dLdw, dLdb = ML_DL_Functions2.derivative_cost(X, y, t)\n",
        "\n",
        "            # Update w and b (gradient descent step)\n",
        "            w = w - mu * dLdw\n",
        "            b = b - mu * dLdb\n",
        "\n",
        "            # Increment the iteration count\n",
        "            iter += 1\n",
        "\n",
        "            # Compute and print the *validation* loss and accuracy\n",
        "            if (iter % 40 == 0):\n",
        "                val_y = ML_DL_Functions2.pred(w, b, val_norm_xs)\n",
        "                val_cost = ML_DL_Functions2.cost(val_y, val_ts.flatten())\n",
        "                val_acc = ML_DL_Functions2.get_accuracy(val_y, val_ts.flatten())\n",
        "                cost_list.append(val_cost)\n",
        "                acc_list.append(val_acc)\n",
        "\n",
        "                # Save the best weights and biases\n",
        "                if val_acc > max_acc:\n",
        "                    max_acc = val_acc\n",
        "                    opt_w = w\n",
        "                    opt_b = b\n",
        "\n",
        "                print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (\n",
        "                    iter, val_acc * 100, val_cost))\n",
        "\n",
        "            if iter >= max_iters:\n",
        "                break\n",
        "\n",
        "    return opt_w, opt_b, cost_list, acc_list\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqzT0jGzGrH"
      },
      "source": [
        "### 2.6 Running everything!\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero. Test your self with different $\\mu$ values and show that if mu is too small then convergance is slow and if mu is too large then the optimization algorithm does not converge. You can add more automation and plot function to help you find the best configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE32Iqo6zGrH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "66c9c3fe-2da1-407d-b12d-0bfd256bc98a"
      },
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize weights and bias\n",
        "w0 = np.zeros(90)  # Initial weights\n",
        "b0 = 0.0           # Initial bias\n",
        "\n",
        "# Choose values\n",
        "mu = 0.1        # Learning rate\n",
        "max_iters = 200    # Maximum number of iterations\n",
        "batch_size = 256   # Batch size\n",
        "\n",
        "# Run gradient descent\n",
        "opt_w, opt_b, cost_list, acc_list = run_gradient_descent(\n",
        "    w0, b0, train_norm_xs, train_ts, val_norm_xs, val_ts, mu, batch_size, max_iters\n",
        ")\n",
        "\n",
        "# Plot classification accuracy per iteration\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(0, max_iters, 40), acc_list, \"r-\", label=\"Accuracy\")\n",
        "plt.title(f\"Classification Accuracy per Iteration; $\\mu$={mu}, Batch Size={batch_size}\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 40. [Val Acc 70%, Loss 0.610055]\n",
            "Iter 80. [Val Acc 71%, Loss 0.591303]\n",
            "Iter 120. [Val Acc 72%, Loss 0.581900]\n",
            "Iter 160. [Val Acc 72%, Loss 0.575193]\n",
            "Iter 200. [Val Acc 72%, Loss 0.571631]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5HlDFAzGrH"
      },
      "source": [
        "### 2.7 Finding and saving optimal values\n",
        "\n",
        "Find the optimal value of ${\\bf w}$ and $b$ in the means of accuracy using your code. Notice that the choice of $\\mu$ and the batch size are important for this. Run the code below to save these parameters to a file in your google drive directory. submit to the moodle (alongside this file and the functions file) both the\n",
        "\"assignment2_submission_optimal_weights.npy\" file and \"assignment2_submission_optimal_bias.npy\" file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFOFSwgzGrI"
      },
      "source": [
        "#change these to the optimal weights and biases, leave the name the same\n",
        "np.save(drive_path+\"assignment2_submission_optimal_weights.npy\",opt_w)\n",
        "np.save(drive_path+\"assignment2_submission_optimal_bias.npy\",opt_b)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrQqSj2zGrI"
      },
      "source": [
        "### 2.8 Results\n",
        "\n",
        "Using the values of `w` and `b` from part 2.7, compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKw2mLozGrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2df336-e201-47ac-e686-ffecae73de98"
      },
      "source": [
        "w = np.load(drive_path+\"assignment2_submission_optimal_weights.npy\")\n",
        "b = np.load(drive_path+\"assignment2_submission_optimal_bias.npy\")\n",
        "\n",
        "# Write your code here\n",
        "\n",
        "train_acc = ML_DL_Functions2.get_accuracy(ML_DL_Functions2.pred(w, b, train_norm_xs), train_ts)\n",
        "val_acc = ML_DL_Functions2.get_accuracy(ML_DL_Functions2.pred(w, b, val_norm_xs), val_ts)\n",
        "test_acc = ML_DL_Functions2.get_accuracy(ML_DL_Functions2.pred(w, b, test_norm_xs), test_ts)\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_acc =  0.7249362483835491  val_acc =  0.72376  test_acc =  0.7182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.9 Using Pytorch\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package. The following example showes you how this task could have been achieved using the deep learning library, pytorch. The library greatly simplifies the steps needed to create a learning model. Though there is nothing you need to complete in this section we suggest you read this section thoroughly and make sure you understand all the code. In the next assignment you will need to build a deep learning model yourself."
      ],
      "metadata": {
        "id": "gO-V4yYACAtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step required to use the pytorch module is to create a class which will be our model. in this case we will use a linear layer with a costum size(in your assignment you used a 90,1 linear layer meaning an input size of 90 and an output size of 1). We also add a sigmoid function to restrict the values between 0 and 1.\n",
        "\n",
        "The forward function is called everytime you call the model by name. It is equivalent to the prediction function you wrote but it serves another purpose since it saves all the operations done to the tensor which can then be used to calculate the gradients."
      ],
      "metadata": {
        "id": "STfBYRNUGESO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class single_layer(torch.nn.Module):\n",
        "  def __init__(self,input_size,output_size):\n",
        "    super(single_layer,self).__init__()\n",
        "    self.neuron = torch.nn.Linear(input_size,output_size)\n",
        "    self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "  def forward(self,X):\n",
        "    out = self.neuron(X)\n",
        "    out = self.sigmoid(out)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "nFsHov0TCYzU"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a new model.\n",
        "\n",
        "we don't have to write the binary cross entropy loss since it is already written for us(criterion).\n",
        "\n",
        "Also instead of writing the optimzation proccess which in our case was gradient descent(W[n+1] = w[n]-$\\mu$dL/dW) we can use a pre built optimizer(SGD)."
      ],
      "metadata": {
        "id": "xD7R_h_DHHxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = single_layer(90,1)\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr = 0.05)"
      ],
      "metadata": {
        "id": "tB3DPFRfHB8s"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few pre-built training functions but usually the training function is written by hand. this function is similar to the one you wrote in this assignment only we now can use the pre-built tensor functions. Make sure you understood all the differences between the two:"
      ],
      "metadata": {
        "id": "4lVhHsxKISRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "def train_model(model, criterion, optimizer, batch_size=100, max_iters=100):\n",
        "  iter = 0\n",
        "  cost_list = []\n",
        "  acc_list  = []\n",
        "  train_norm_xs_shuff = train_norm_xs\n",
        "  train_ts_shuff = train_ts\n",
        "  val_X_tensor = torch.tensor(val_norm_xs,dtype=torch.float32)\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    reindex = np.random.permutation(len(train_norm_xs))\n",
        "    train_norm_xs_shuff = train_norm_xs_shuff[reindex]\n",
        "    train_ts_shuff = train_ts_shuff[reindex]\n",
        "\n",
        "    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_norm_xs_shuff[i:(i + batch_size)]\n",
        "      t = train_ts_shuff[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "      # change the numpy types into torches tensors\n",
        "      X_tensor = torch.tensor(X,dtype=torch.float32)\n",
        "      t_tensor = torch.tensor(t,dtype=torch.float32).unsqueeze(1) # the unsqueeze reshapes (N,) to (N,1)\n",
        "\n",
        "      # a clean up step for PyTorch\n",
        "      optimizer.zero_grad()\n",
        "      # compute the prediction\n",
        "      prediction = model(X_tensor)\n",
        "      # compute the cost/loss\n",
        "      loss = criterion(prediction,t_tensor)\n",
        "      # calculate gradient(backpropegate)\n",
        "      loss.backward()\n",
        "      # update w and b(step)\n",
        "      optimizer.step()\n",
        "      # increment the iteration count\n",
        "      iter += 1\n",
        "      # compute and print the *validation* accuracy\n",
        "      if (iter % 40 == 0):\n",
        "        val_pred = model(val_X_tensor)\n",
        "        val_acc = ML_DL_Functions2.get_accuracy(val_pred,val_ts)\n",
        "        acc_list.append(val_acc)\n",
        "\n",
        "        print(\"Iter %d. [Val Acc %.1f%%]\" % (\n",
        "                iter, val_acc * 100))\n",
        "\n",
        "      if iter >= max_iters:\n",
        "        break\n",
        "\n",
        "\n",
        "  return acc_list\n"
      ],
      "metadata": {
        "id": "goyjkL11D5OA"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run the training proccess. You should get pretty similar results to the ones from the model you wrote. Make sure that the results are in the same range and if not fix your model and try again."
      ],
      "metadata": {
        "id": "2XTEVQkgJu0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reload_functions()\n",
        "import ML_DL_Functions2\n",
        "acc_list = train_model(model,criterion,optimizer,100,500)"
      ],
      "metadata": {
        "id": "1ajs1myYKl4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdecaa5a-332a-4c4a-93db-fd3b93dad106"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 40. [Val Acc 65.7%]\n",
            "Iter 80. [Val Acc 68.9%]\n",
            "Iter 120. [Val Acc 70.3%]\n",
            "Iter 160. [Val Acc 71.0%]\n",
            "Iter 200. [Val Acc 71.1%]\n",
            "Iter 240. [Val Acc 71.8%]\n",
            "Iter 280. [Val Acc 71.6%]\n",
            "Iter 320. [Val Acc 72.1%]\n",
            "Iter 360. [Val Acc 72.2%]\n",
            "Iter 400. [Val Acc 72.2%]\n",
            "Iter 440. [Val Acc 72.5%]\n",
            "Iter 480. [Val Acc 72.5%]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also try to change the model(add layers or change layers) change the optimizer or the hyperparameters and try to improve the validation accuracy. If you want a challenge you can try reach a validation accuracy of 75%"
      ],
      "metadata": {
        "id": "XFj6EzWAB7ry"
      }
    }
  ]
}
